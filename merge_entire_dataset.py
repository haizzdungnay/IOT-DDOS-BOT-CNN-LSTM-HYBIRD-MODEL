#!/usr/bin/env python3
"""
Script h·ª£p nh·∫•t dataset Entire Bot-IoT v√† l·∫•y t·∫•t c·∫£ features c√≥ s·∫µn
T·ª± ƒë·ªông kh√°m ph√° c·∫•u tr√∫c d·ªØ li·ªáu
"""

import pandas as pd
import os
from pathlib import Path
import json

# ƒê∆∞·ªùng d·∫´n
entire_dir = r"E:\Bot_IOT_Dataset\Dataset\Dataset\Entire Dataset"

# L·∫•y danh s√°ch file
csv_files = sorted([f for f in os.listdir(entire_dir) if f.endswith('.csv') and 'Dataset_' in f])

print("=" * 60)
print("KH√ÅM PH√Å V√Ä H·ª¢P NH·∫§T DATASET ENTIRE BOT-IOT")
print("=" * 60)
print(f"\n[INFO] T√¨m ƒë∆∞·ª£c {len(csv_files)} file CSV")

if not csv_files:
    print("‚ùå Kh√¥ng t√¨m th·∫•y file CSV!")
    exit(1)

# ƒê·ªçc file ƒë·∫ßu ti√™n ƒë·ªÉ kh√°m ph√° c·∫•u tr√∫c
first_file = csv_files[0]
print(f"\n[1] Kh√°m ph√° c·∫•u tr√∫c t·ª´ {first_file}...")

df_sample = pd.read_csv(Path(entire_dir) / first_file, nrows=1000)
print(f"    Columns ({len(df_sample.columns)}):")
for i, col in enumerate(df_sample.columns, 1):
    print(f"      {i:2}. {col}")

# T√¨m label column
label_col = None
for name in ['attack', 'label', 'Label', 'ATTACK', 'Attack', 'class']:
    if name in df_sample.columns:
        label_col = name
        break

if label_col:
    print(f"\n‚úÖ T√¨m th·∫•y label column: '{label_col}'")
else:
    print(f"\n‚ö†Ô∏è  Kh√¥ng t√¨m th·∫•y label column! D√πng 'attack'")
    label_col = 'attack'

# L·∫•y t·∫•t c·∫£ feature (tr·ª´ label)
all_features = [col for col in df_sample.columns if col != label_col]
print(f"\nüìä T·ªïng features: {len(all_features)}")

# H·ª£p nh·∫•t (l·∫•y subset ƒë·ªÉ nhanh)
print(f"\n[2] H·ª£p nh·∫•t {len(csv_files)} files...")
print("    ‚è≥ ƒêang ƒë·ªçc... (m·ªói file ~200-250MB)")

dfs = []
total_normal = 0
total_attack = 0

for i, filename in enumerate(csv_files, 1):
    if i % 10 == 0 or i == len(csv_files):
        print(f"    [{i}/{len(csv_files)}] {filename}")
    
    filepath = Path(entire_dir) / filename
    try:
        df = pd.read_csv(filepath)
        
        # Th·ªëng k√™
        if label_col in df.columns:
            n = (df[label_col] == 0).sum()
            a = (df[label_col] == 1).sum()
            total_normal += n
            total_attack += a
        
        dfs.append(df)
    except Exception as e:
        print(f"    ‚ö†Ô∏è  L·ªói ƒë·ªçc {filename}: {e}")

print(f"\n    ƒê√£ load {len(dfs)} files")
print(f"    Normal: {total_normal:,}")
print(f"    Attack: {total_attack:,}")

# H·ª£p nh·∫•t
print(f"\n[3] H·ª£p nh·∫•t {len(dfs)} files th√†nh 1 DataFrame...")
merged_df = pd.concat(dfs, ignore_index=True)

print(f"    Shape: {merged_df.shape}")
print(f"    Features: {len(merged_df.columns)}")

# L∆∞u th√¥ng tin c·∫•u h√¨nh
config_data = {
    "features": all_features,
    "label_column": label_col,
    "total_samples": len(merged_df),
    "normal_count": total_normal,
    "attack_count": total_attack,
    "columns": list(merged_df.columns)
}

config_path = Path(entire_dir) / "dataset_config.json"
with open(config_path, 'w') as f:
    json.dump(config_data, f, indent=2)
print(f"\n‚úÖ L∆∞u config: {config_path}")

# L∆∞u merged
output_path = Path(entire_dir) / "UNSW_2018_IoT_Botnet_Entire_Merged.csv"
print(f"\n[4] L∆∞u merged file...")
print(f"    {output_path}")
print("    ‚è≥ ƒêang ghi file... (m·∫•t ~5-10 ph√∫t)")

merged_df.to_csv(output_path, index=False)

file_size_gb = os.path.getsize(output_path) / 1024 / 1024 / 1024
print(f"\n‚úÖ Ho√†n th√†nh!")
print(f"    File size: {file_size_gb:.2f} GB")
print(f"    Total rows: {len(merged_df):,}")
print(f"    Normal: {total_normal:,} ({total_normal/len(merged_df)*100:.2f}%)")
print(f"    Attack: {total_attack:,} ({total_attack/len(merged_df)*100:.2f}%)")

print(f"\nüìå B∆∞·ªõc ti·∫øp theo:")
print(f"   1. C·∫≠p nh·∫≠t training/config.py v·ªõi features m·ªõi")
print(f"   2. Training: python training/train_all.py --data \"{output_path}\" --models LSTM HYBRID")
